# BEGIN SNAKEFILE 
# mixarray.snakefile
# Credit to Gavin Ha, Eden Cruikshank, Anna-Lisa Doebley and Anna Hoge for various contributions
# Matt Neel
# March 17, 2021
# Ha Lab
# Fred Hutchinson Cancer Research Center

"""
# Before running snakemake, module load these packages in the terminal:
ml snakemake/5.19.2-foss-2019b-Python-3.7.4
ml BWA/0.7.17-foss-2018b
ml SAMtools/1.10-GCCcore-8.3.0
ml GATK/4.1.4.1-GCCcore-8.3.0-Java-11
ml picard/2.18.29-Java
ml R/4.0.3-foss-2020b 

# Use this command to run snakemake (remove -np at end when done validating):
snakemake -s mixing.snakefile --latency-wait 60 --keep-going --cluster-config config/cluster_slurm.yaml --cluster "sbatch -p {cluster.partition} --mem={cluster.mem} -t {cluster.time} -c {cluster.ncpus} -n {cluster.ntasks} -o {cluster.output} -J {cluster.JobName}" -j 40 -np
"""

configfile: "config/samples.yaml"
configfile: "config/config.yaml"

# ---------------------------------------
# Rule that lists all the output files generated by the subsequent rules
rule all:
	input: 
		# Create the prepared source files once; they'll be used many times
		expand("results/{mixset}/sources/H-chrsOnly.bam", mixset=config["samples"]),
		expand("results/{mixset}/sources/T-chrsOnly.bam", mixset=config["samples"]),
		expand("results/{mixset}/sources/H-chrsOnly.dups_removed.bam", mixset=config["samples"]),
		expand("results/{mixset}/sources/H-chrsOnly.dups_removed_metrics.txt", mixset=config["samples"]),
		expand("results/{mixset}/sources/T-chrsOnly.dups_removed.bam", mixset=config["samples"]),
		expand("results/{mixset}/sources/T-chrsOnly.dups_removed.metrics.txt", mixset=config["samples"]),
		expand("results/{mixset}/sources/H-chrsOnly.dups_removed.alignment_summary_metrics.txt", mixset=config["samples"]),
		expand("results/{mixset}/sources/T-chrsOnly.dups_removed.alignment_summary_metrics.txt", mixset=config["samples"]),

		# Do this part once for each TF increment in the range specified
		expand("results/{mixset}/{tf}/{mixset}_{tf}_calcProbs.txt", mixset=config["samples"], tf=config["fractions"]),
		expand("results/{mixset}/{tf}/{mixset}_{tf}_H-CODRdownsample.bam", mixset=config["samples"], tf=config["fractions"]),
		expand("results/{mixset}/{tf}/{mixset}_{tf}_T-CODRdownsample.bam", mixset=config["samples"], tf=config["fractions"]),
		expand("results/{mixset}/{tf}/{mixset}_{tf}_THmerge.bam", mixset=config["samples"], tf=config["fractions"]),
		expand("results/{mixset}/{tf}/{mixset}_{tf}_THmerge.bam.bai", mixset=config["samples"], tf=config["fractions"]),
		expand("results/{mixset}/{tf}/{mixset}_{tf}_THmerge.alignment_summary_metrics.txt", mixset=config["samples"], tf=config["fractions"])

# ---------------------------------------
# These rules extract chromosomal DNA from the source bam files using samtools
rule extract_healthy:
	input:
		inBam = lambda wildcards: config["samples"][wildcards.mixset][1]
	output:
		outBam = "results/{mixset}/sources/H-chrsOnly.bam"
	params:
		sam = config["samtools"],
		chrs = config["chrs"]
	log:
		logfile = "logs/{mixset}/{mixset}_H-extractChrs.txt"
	shell:
		"({params.sam} view -b {input.inBam} {params.chrs} > {output.outBam}) 2> {log.logfile}"

rule extract_tumor:
	input:
		inBam = lambda wildcards: config["samples"][wildcards.mixset][0]
	output:
		outBam = "results/{mixset}/sources/T-chrsOnly.bam"
	params:
		sam = config["samtools"],
		chrs = config["chrs"]
	log:
		logfile = "logs/{mixset}/{mixset}_T-extractChrs.txt"
	shell:
		"({params.sam} view -b {input.inBam} {params.chrs} > {output.outBam}) 2> {log.logfile}"

# ---------------------------------------
# These rules remove duplicates after the chromosomal extraction above using Picard tools (java)
rule removeduplicates_healthy:
	input:
		inBam = "results/{mixset}/sources/H-chrsOnly.bam"
	output:
		outBam = "results/{mixset}/sources/H-chrsOnly.dups_removed.bam",
		metrics = "results/{mixset}/sources/H-chrsOnly.dups_removed_metrics.txt"
	params:
		mem = config["java_mem"],
		picard = config["picard"]
	log:
		logfile = "logs/{mixset}/{mixset}_H-removeDuplicates.txt"
	shell:
		"(java {params.mem} -jar {params.picard} MarkDuplicates \
		INPUT={input.inBam} \
		OUTPUT={output.outBam} \
		METRICS_FILE={output.metrics} \
		REMOVE_DUPLICATES=true \
		CREATE_INDEX=false \
		ASSUME_SORT_ORDER=coordinate \
		VALIDATION_STRINGENCY=LENIENT \
		TMP_DIR=temp) 2> {log.logfile}"

rule removeduplicates_tumor:
	input:
		inBam = "results/{mixset}/sources/T-chrsOnly.bam"
	output:
		outBam = "results/{mixset}/sources/T-chrsOnly.dups_removed.bam",
		metrics = "results/{mixset}/sources/T-chrsOnly.dups_removed.metrics.txt"
	params:
		mem = config["java_mem"],
		picard = config["picard"]
	log:
		logfile = "logs/{mixset}/{mixset}_T-removeDuplicates.txt"
	shell:
		"(java {params.mem} -jar {params.picard} MarkDuplicates \
		INPUT={input.inBam} \
		OUTPUT={output.outBam} \
		METRICS_FILE={output.metrics} \
		REMOVE_DUPLICATES=true \
		CREATE_INDEX=false \
		ASSUME_SORT_ORDER=coordinate \
		VALIDATION_STRINGENCY=LENIENT \
		TMP_DIR=temp) 2> {log.logfile}"

# ---------------------------------------
# These rules get alignment metrics for the extracted/de-duplicated bams above
rule createCODRmetrics_healthy:
	input:
		inBam = "results/{mixset}/sources/H-chrsOnly.dups_removed.bam"
	output:
		metrics = "results/{mixset}/sources/H-chrsOnly.dups_removed.alignment_summary_metrics.txt"
	params:
		mem = config["java_mem"],
		picard = config["picard"],
		ref = config["ref_genome"]
	log:
		logfile = "logs/{mixset}/{mixset}_H-CODR-metrics.txt"
	shell:
		"(java {params.mem} -jar {params.picard} CollectAlignmentSummaryMetrics \
		R={params.ref} \
		I={input.inBam} \
		O={output.metrics} \
		VALIDATION_STRINGENCY=LENIENT) 2> {log.logfile}"

rule createCODRmetrics_tumor:
	input:
		inBam = "results/{mixset}/sources/T-chrsOnly.dups_removed.bam"
	output:
		metrics = "results/{mixset}/sources/T-chrsOnly.dups_removed.alignment_summary_metrics.txt"
	params:
		mem = config["java_mem"],
		picard = config["picard"],
		ref = config["ref_genome"]
	log:
		logfile = "logs/{mixset}/{mixset}_T-CODR-metrics.txt"
	shell:
		"(java {params.mem} -jar {params.picard} CollectAlignmentSummaryMetrics \
		R={params.ref} \
		I={input.inBam} \
		O={output.metrics} \
		VALIDATION_STRINGENCY=LENIENT) 2> {log.logfile}"

# ---------------------------------------
# This rule takes in various data to calculate the downsampling probabilities for the tumor / healthy bams
# Note that the perl script referenced here will produce a file no matter what, but the contents of that
# file will vary. If the user has asked for a coverage or tumor fraction that is not possible with the
# given samples, the output file will list the reason(s) with various supporting values but NOT the two
# lines that the downsampling rules that follow will be looking for. If the coverage / tumor fraction
# are possible, the output file will have the two lines needed for the downsampling rules.
rule create_probabilities:
	input:
		inTumor = "results/{mixset}/sources/T-chrsOnly.dups_removed.alignment_summary_metrics.txt",
		inHealthy = "results/{mixset}/sources/H-chrsOnly.dups_removed.alignment_summary_metrics.txt"
	output:
		outFile = "results/{mixset}/{tf}/{mixset}_{tf}_calcProbs.txt"
	params:
		cover = lambda wildcards: config["samples"][wildcards.mixset][3],
		TF = lambda wildcards: config["fractions"][wildcards.tf],
		TPurity = lambda wildcards: config["samples"][wildcards.mixset][2]
	log:
		logfile = "logs/{mixset}/{mixset}_{tf}_probabilities.txt"
	shell:
		"(perl calcProbabilities.pl \
		-inFileT {input.inTumor} \
		-inFileH {input.inHealthy} \
		-cover {params.cover} \
		-tumorFrac {params.TF} \
		-purity {params.TPurity} \
		-outFile {output.outFile}) 2> {log.logfile}"

# ---------------------------------------
# These rules use the probabilities in the previous step to downsample the healthy / tumor samples accordingly
# If these rules trigger and fail, the reason may be that the user is asking for coverage / tumor fraction
# that is not possible with these samples. Look in the *_calcProbs.txt file produced in the previous step
# If there are no lines in that file beginning with TUMORPROB or HEALTHYPROB, then the prior step determined
# there was an error in the requested values
rule downsample_healthy:
	input:
		inBam = "results/{mixset}/sources/H-chrsOnly.dups_removed.bam",
		probfile = "results/{mixset}/{tf}/{mixset}_{tf}_calcProbs.txt"
	output:
		outBam = "results/{mixset}/{tf}/{mixset}_{tf}_H-CODRdownsample.bam"
	params:
		mem = config["java_mem"],
		picard = config["picard"]
	log:
		logfile = "logs/{mixset}/{mixset}_{tf}_H-downsample.txt"
	shell:
		"HPROB=$(grep HEALTHYPROB {input.probfile} | cut -f2);"
		"(java {params.mem} -jar {params.picard} DownsampleSam \
		I={input.inBam} \
		O={output.outBam} \
		CREATE_INDEX=false \
		PROBABILITY=$HPROB \
		STRATEGY=HighAccuracy \
		ACCURACY=1.0E-4) 2> {log.logfile}"

rule downsample_tumor:
	input:
		inBam = "results/{mixset}/sources/T-chrsOnly.dups_removed.bam",
		probfile = "results/{mixset}/{tf}/{mixset}_{tf}_calcProbs.txt"
	output:
		outBam = "results/{mixset}/{tf}/{mixset}_{tf}_T-CODRdownsample.bam"
	params:
		mem = config["java_mem"],
		picard = config["picard"]
	log:
		logfile = "logs/{mixset}/{mixset}_{tf}_T-downsample.txt"
	shell:
		"TPROB=$(grep TUMORPROB {input.probfile} | cut -f2);"
		"(java {params.mem} -jar {params.picard} DownsampleSam \
		I={input.inBam} \
		O={output.outBam} \
		CREATE_INDEX=false \
		PROBABILITY=$TPROB \
		STRATEGY=HighAccuracy \
		ACCURACY=1.0E-4) 2> {log.logfile}"

# ---------------------------------------
# Now that the original samples have been chrom-extracted, de-duplicated and downsampled...
# Let's merge the results, create a symlink and provide metrics on the merged bam
rule mergefiles:
	input:
		inBamH = "results/{mixset}/{tf}/{mixset}_{tf}_H-CODRdownsample.bam",
		inBamT = "results/{mixset}/{tf}/{mixset}_{tf}_T-CODRdownsample.bam"
	output:
		outBam = "results/{mixset}/{tf}/{mixset}_{tf}_THmerge.bam",
		outBai = "results/{mixset}/{tf}/{mixset}_{tf}_THmerge.bai"
	params:
		mem = config["java_mem"],
		picard = config["picard"]
	log:
		logfile = "logs/{mixset}/{mixset}_{tf}_TH-merge.txt"
	shell:
		"(java {params.mem} -jar {params.picard} MergeSamFiles \
		I={input.inBamH} \
		I={input.inBamT} \
		O={output.outBam} \
		CREATE_INDEX=true \
		USE_THREADING=true) 2> {log.logfile}"

# Note that this rule has an input file but does not actually use it
# This is by design to ensure a relative-path symlink is created using the path in params
rule create_symlinks:
	input:
		inBai = "results/{mixset}/{tf}/{mixset}_{tf}_THmerge.bai"
	output:
		outBamBai = "results/{mixset}/{tf}/{mixset}_{tf}_THmerge.bam.bai"
	params:
		relpath = "{mixset}_{tf}_THmerge.bai"
	log:
		logfile = "logs/{mixset}/{mixset}_{tf}_TH-symlink.txt"
	shell:
		"(ln -s {params.relpath} {output.outBamBai}) 2> {log.logfile}"

rule create_merge_metrics:
	input:
		inBam = "results/{mixset}/{tf}/{mixset}_{tf}_THmerge.bam"
	output:
		metrics = "results/{mixset}/{tf}/{mixset}_{tf}_THmerge.alignment_summary_metrics.txt"
	params:
		mem = config["java_mem"],
		picard = config["picard"],
		ref = config["ref_genome"]
	log:
		logfile = "logs/{mixset}/{mixset}_{tf}_THmerge-metrics.txt"
	shell:
		"(java {params.mem} -jar {params.picard} CollectAlignmentSummaryMetrics \
		R={params.ref} \
		I={input.inBam} \
		O={output.metrics} \
		VALIDATION_STRINGENCY=LENIENT) 2> {log.logfile}"

# END SNAKEFILE
